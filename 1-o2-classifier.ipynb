{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8385c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import *\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "n_iterations = 10000\n",
    "feature_proportion = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "654902bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (1, 1)},\n",
       " {'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (2, 2)},\n",
       " {'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (3, 3)},\n",
       " {'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (4, 4)},\n",
       " {'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (1, 1)},\n",
       " {'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (2, 2)},\n",
       " {'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (3, 3)},\n",
       " {'use_idf': True,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (4, 4)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (1, 1)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (2, 2)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (3, 3)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'word',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (4, 4)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (1, 1)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (2, 2)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (3, 3)},\n",
       " {'use_idf': False,\n",
       "  'max_features': 10000,\n",
       "  'analyzer': 'char',\n",
       "  'min_df': 2,\n",
       "  'lowercase': True,\n",
       "  'norm': 'l1',\n",
       "  'ngram_range': (4, 4)}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of vectorizer settings - each one will create\n",
    "# a new feature space from the corpus. \n",
    "\n",
    "vectorizers_settings = []\n",
    "idf_settings = [True, False]\n",
    "ngram_ranges = range(1,5)\n",
    "analyzers = ['word', 'char']\n",
    "for idf_setting in idf_settings: \n",
    "    for analyzer in analyzers: \n",
    "        for ngram_range in ngram_ranges: \n",
    "            vectorizer_setting= {'use_idf': idf_setting,\n",
    "                  'max_features': 10000,\n",
    "                  'analyzer': analyzer,\n",
    "                  'min_df': 2,\n",
    "                  'lowercase': True,\n",
    "                  'norm': 'l1',\n",
    "                  'ngram_range': (ngram_range, ngram_range)}\n",
    "            vectorizers_settings.append(vectorizer_setting)\n",
    "\n",
    "print(len(vectorizers_settings))\n",
    "vectorizers_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e064b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ LOADING SEGMENTS ################\n",
      "1000\n",
      "     author     date          title  segment_number  \\\n",
      "0   bachman  1966.67  The_Long_Walk               0   \n",
      "1   bachman  1966.67  The_Long_Walk               1   \n",
      "12  bachman  1966.67  The_Long_Walk               2   \n",
      "23  bachman  1966.67  The_Long_Walk               3   \n",
      "34  bachman  1966.67  The_Long_Walk               4   \n",
      "\n",
      "                                                 text  \n",
      "0   part one starting out the long walk chapter 1 ...  \n",
      "1   hand and waved the tears were flowing now he c...  \n",
      "12  the major said sweeping them with the blank le...  \n",
      "23  nt he had forgotten the major s fingers droppe...  \n",
      "34  in garraty s belly that felt like a sticky bal...  \n",
      "\n",
      "################ GETTING SEGMENT FEATURE SPACES ################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [02:39<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################ GETTING COSINE DISTANCES ################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 603/603 [10:21:42<00:00, 61.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################ SAVING DISTANCES ################\n",
      "   target index  bootstrap iteration candidate  distance\n",
      "0             0                    0    koontz  0.692278\n",
      "1             0                    0      king  0.708261\n",
      "2             0                    0    straub  0.676044\n",
      "3             0                    0    harris  0.670621\n",
      "4             0                    1    koontz  0.716662\n",
      "\n",
      "################ SAVING RANKS ################\n"
     ]
    }
   ],
   "source": [
    "# segment_lengths = [1000, 5000, 10000]\n",
    "segment_lengths = [1000]\n",
    "\n",
    "for segment_length in segment_lengths:\n",
    "    \n",
    "    print(\"################ LOADING SEGMENTS ################\")\n",
    "    print(segment_length)\n",
    "    authors, dates, titles, segment_numbers, texts = [], [], [], [], []\n",
    "    os.chdir(top_dir + \"{}token_segments\".format(segment_length))\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\"):\n",
    "            authors.append(file.split('-')[0])\n",
    "            dates.append(file.split('-')[1])\n",
    "            titles.append(file.split('-')[2])\n",
    "            segment_numbers.append(file.split('-')[3].split(\".\")[0])\n",
    "            with open(file, encoding='utf8') as f:\n",
    "                contents = f.read()\n",
    "                texts.append(contents)\n",
    "    \n",
    "    authors_segments = pd.DataFrame({\n",
    "        'author': authors,\n",
    "        'date':dates, \n",
    "        'title':titles,\n",
    "        'segment_number': segment_numbers,\n",
    "        'text':texts})\n",
    "\n",
    "    # Convert segment number column to numeric so dataframe rows can be sorted\n",
    "    authors_segments.segment_number = pd.to_numeric(authors_segments.segment_number)\n",
    "    authors_segments = authors_segments.sort_values(by=['author', 'date', 'segment_number'])\n",
    "    print(authors_segments.head())\n",
    "    \n",
    "    authors_features = authors_segments[['author', \n",
    "                                     'date', \n",
    "                                     'title', \n",
    "                                     'segment_number']]\n",
    "    # Iterate over vectorizer settings, make feature spaces,\n",
    "    # append each new feature space to the dataframe authors_features. \n",
    "    # Number of rows = number of segments\n",
    "    # Number of columns = number of columns created by 1st vectorizer + number of columns created by 2nd vectorizer + ... \n",
    "    feature_spaces = []\n",
    "    \n",
    "    print(\"\\n################ GETTING SEGMENT FEATURE SPACES ################\")\n",
    "    for vectorizer_setting in tqdm(vectorizers_settings): \n",
    "        vectorizer = TfidfVectorizer(**vectorizer_setting)\n",
    "        docterm_matrix = vectorizer.fit_transform(authors_segments.text).toarray()\n",
    "        scaler = MinMaxScaler()\n",
    "        feature_spaces.append(scaler.fit_transform(docterm_matrix))\n",
    "    \n",
    "    feature_spaces_array = np.hstack(feature_spaces)\n",
    "    \n",
    "    candidates = {}\n",
    "    for author in (\"koontz\", \"king\", \"straub\", \"harris\"):\n",
    "        candidates[author] = feature_spaces_array[authors_features['author'] == author]\n",
    "    \n",
    "    bachman = feature_spaces_array[authors_features['author'] == 'bachman']\n",
    "    \n",
    "    num_random_features = int(feature_spaces_array.shape[1]*feature_proportion)\n",
    "    \n",
    "    # Get distances of every Bachman segment to segments from other authors 10,000 times\n",
    "    print(\"\\n################ GETTING COSINE DISTANCES ################\")\n",
    "    results = []\n",
    "    for idx, target_segment in tqdm(list(enumerate(bachman))): \n",
    "        for iteration in range(n_iterations):\n",
    "            rand_feature_col_idxs = np.random.choice(feature_spaces_array.shape[1], num_random_features, replace=False)\n",
    "            target_vector = target_segment[rand_feature_col_idxs]\n",
    "\n",
    "            for candidate in candidates: \n",
    "                random_segment = candidates[candidate][np.random.choice(candidates[candidate].shape[0]), rand_feature_col_idxs]\n",
    "                results.append((idx, iteration, candidate, cosine(target_vector, random_segment)))\n",
    "    \n",
    "    print(\"\\n################ SAVING DISTANCES ################\")\n",
    "    df = pd.DataFrame(results, columns = ('target index', 'bootstrap iteration', 'candidate', 'distance'))\n",
    "    print(df.head())\n",
    "    os.chdir(top_dir)\n",
    "    df.to_csv(\"bachman_segments_candidate_distance_{}tok.csv\".format(segment_length), encoding='utf-8')\n",
    "    \n",
    "    print(\"\\n################ SAVING RANKS ################\")\n",
    "    pivoted_df = df.pivot(index=['target index', 'bootstrap iteration'], columns='candidate')['distance']\n",
    "    pivoted_df = pivoted_df.rank(1)\n",
    "    pivoted_df.to_csv(\"bachman_segments_candidate_rank_{}tok.csv\".format(segment_length), encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
